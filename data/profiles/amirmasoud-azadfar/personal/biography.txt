My name is AmirMasoud Azadfar. I was born in April of 1999 in Iran and am currently living in Thunder Bay, Ontario, Canada since November of 2023. I'm studying an honors bachelors degree in Computer Science at Lakehead University. You might think, as a computer science bachelors student I wouldn't know much of tech and programming, you might be right, but in this case, you're wrong. My journey into the tech world has been filled with twists, turns, and a lot of learning. I'm going to tell you my story so that you get to know me and how I entered the tech field when I was a child and what I did and learned throughout these years. Everything starts with my father, who is an electrical engineer with expertise in computer programming. He introduced me to the world of computers and programming when I was a child. I was always curious about how things worked and how I could make them better, striving for perfection and efficiency. At 10, my father taught me data structures, algorithms, and flowcharts. He then taught me C programming language in Visual Studio 2008. I remember one of my first console applications was a program to calculate Napier's constant using the Taylor expansion. This experience was a pivotal moment in my life, giving me a glimpse of the career path I would eventually follow.

During middle school, from 2010 to 2013, I dived deeply into programming ang applying it to real world projects. By 2010, when I was 11 years old, I began learning C++ for object-oriented programming. I often explored my father's notes and booklets, which he used for teaching microprocessor principles at the university as a professor. In middle school, we were taught the BASIC programming language. I remember feeling bored in class because I already had a good grasp of data structures, algorithms, and programming principles. My peers nicknamed me "C Plus Plus" after I asked the computer teacher on orientation day if we would be learning the C++ language, which was well beyond the standard middle school curriculum. In 2011 I entered a national programming competition called HelliNet and placed first in the state. My passion for programming led me to learn how to program AVR microcontrollers in C using CodeVisionAVR. The first functional robot I built was a fighting robot, which used four DC motors, a radio frequency remote control, and an ATmega8 AVR microcontroller. I then delved deeper into robotics, building a high-speed line follower robot from scratch using 16 IR sensors and an ATmega32 AVR microcontroller. The robot was incredibly fast. Alongside AVR programming, I also taught myself the MFC (Microsoft Foundation Classes) framework for C++. In 2012, given my experience with MFC, C++, robotics, and microcontrollers, I proposed to two of my friends that we enter the Sharif University of Technology's Robotics Competition at the university level. We aimed to build a smart gardener robot, and I was responsible for developing the core software. I implemented machine vision algorithms, image processing, edge detection, and control commands (e.g., pathfinding, obstacle avoidance, barcode scanning, water pump control) using C++ and MFC. We deployed the software on an Intel Atom-based MINI ITX motherboard with a custom-made PCB, mounted on a 4-wheel differential drive system with a water container and a USB VGA camera. The camera captured video feed via the serial port to the motherboard, enabling the decision-making software. I spent the entire summer writing the path detection part of the software. We placed third in the competition, which was a significant achievement considering we were three 13-year-olds competing against bachelor's and master's students from the most reputable university in Iran. Later that year, in response to a request from my math teacher, I built a calculator for set operations as a teaching tool. I submitted this software to the Kharazmi Technical and Vocational Handicrafts Festival and placed first nationally. In the third grade, we were taught Visual Basic programming language, but I found the language simplistic compared to the power of MFC and C++, leading me to become bored in class. Nonetheless, I acknowledged its efficiency. In the same year, I enrolled in the CompTIA Network+ course and got familiar with networking.

From 2014 to 2017, when I was in high school, I was literally crushing it. In the first grade of high school, in 2014, I was introduced to a new field of science: neuroscience. Its interdisciplinary nature intrigued me, so I decided to explore it further. I attended numerous research seminars and congresses, which deepened my interest in the field. Eventually, my friends and I decided to participate in IPM's National Cognitive Neuroscience Research Competition. Throughout the first round of the competition from 2015 to 2016, I assembled a 2-channel EMG spiker box PCB for measuring nerve electrical signals using a DIY kit provided by the Backyard Brains team at the University of Michigan. I initially learned a bit of MATLAB for neuron spike signal processing, including techniques like Fourier Transform, Wavelet Transform, and Principal Component Analysis. However, I didn't particularly enjoy using MATLAB, so I decided to learn Python for the same purposes. As a hobby research project, our team built a small pulse generator circuit board to control a cockroach's movements via electrical stimulation through narrow electrodes inserted into its neural ganglia. It was fascinating to control a living organism like a remote-controlled machine, with potential applications in search and rescue scenarios, among others. In 2016, we reached the finals of the competition, presenting our research titled "Measurement of the Effect of Different Electrical Stimulation Frequencies on the Cockroach's Movement and Nerve Adaptation," and placed second in the country. The following year, in 2017, we participated in the competition again. This time, we conducted a "Statistical Analysis of Environmental Stimuli-to-Response Latency in the Cockroach's Nervous System," placing third nationally. I was honored with the National Ahwazi Award for the Best Young Researcher in Cognitive Neuroscience by the Cognitive Sciences and Technologies Council. Also in 2015, while deeply involved in neuroscience, my school's computer teacher asked me to help design control software for a quadcopter as a school project. I wrote the software in C and ran it on an ATmega64 AVR microcontroller, enabling the quadcopter to maintain its altitude and orientation using inputs from a GPS, an angle tilt sensor, an ultrasonic distance sensor, a magnetic compass, and four brushless DC motors. Unfortunately, due to the hardware and design team's lack of responsibility, the software was never tested on the vehicle.

From October 2018 to September 2022, I worked at Sepanta Communications and Processing Information Technology Company as a Data Engineer and Software Developer, where I gained extensive experience in software development and data science. My responsibilities included developing data retrieval and preprocessing pipelines, creating algorithmic trading strategies, and building machine learning models for stock and sentiment analysis. I worked with a range of technologies, including Python, Pandas, NumPy, Scikit-Learn, PyTorch, and MongoDB. My projects often involved real-time data processing, API development, and deploying systems on cloud platforms. This experience honed my skills in software engineering, machine learning, and financial data analysis. 

Here's a detailed list of my responsibilities and accomplishments during my employment at Sepanta:
	- Worked on developing a data retrieval and preprocessing pipeline to collect and process financial technical, fundamental, and news data, as well as real-time technical data for all actively traded stocks on the Tehran Securities Exchange (TSE), which included over 400 stocks. We used Python, leveraging Beautiful Soup and Selenium for web scraping, Regular Expressions and Pandas for data preprocessing and entity extraction, and MongoDB as the NoSQL database for data warehousing. The pipeline successfully collected historical data from the IPO of each stock up to the present and retrieved real-time market data (price, order book, trades, etc.) with a latency of 5000ms. Two years later, we updated this pipeline to include the collection of historical and real-time cryptocurrency market data from Binance and KuCoin exchanges. This update utilized REST API calls for historical data collection and WebSocket streaming for real-time data retrieval. We implemented API key rotation and aggressive rate limiting mechanisms to ensure data reliability, achieving a latency of less than 500ms for real-time data retrieval.
	- Took on a project to develop a consensus clustering model using Scikit-Learn's K-Means and Hierarchical Clustering algorithms on TSE stocks' historical technical and fundamental data for sector and industry analysis. Although the model wasn't highly accurate initially, it was a significant achievement at the time. Over time, we refined and fine-tuned the model, eventually using its outputs as a feature in a more complex statistical analysis.
	- Programmed an algorithmic trading strategy incorporating Mean Reversion, Momentum Trading Strategies, Buyer/Seller Pressure, and Market Microstructure Analysis (Order Book Analysis) for TSE stocks. I also created an option bonds pricing model using the Black-Scholes and Binomial Option Pricing Models. This strategy aimed to identify profitable positions in option bonds, allowing for buying or selling the stock tied to the option to gain profit. I implemented the strategy in Python using Pandas and NumPy.
	- Took on a project to build a sentiment analysis model for TSE stocks news and analyst reports. This was challenging because NLP was still emerging, especially with the rise of models like BERT and transformers, and there was limited work done for languages like Farsi. I had to label the data manually and use some algorithmic approaches. I prepared a dataset of over 15,000 labeled news articles, analyst reports, and comments. I used a library called Hazm for tokenization, text processing, and POS tagging of Persian text. For feature extraction, I implemented TF-IDF and then trained a Naive Bayes classifier. Despite many challenges and fine-tuning, I achieved an F1 score of nearly 0.6, which was a decent outcome at the time. Later, another team built upon this model, turning it into a functional and useful product.
	- Joined a team of developers to build a market analysis and automatic reporting platform for TSE stocks on the Telegram messenger platform. Wanting to learn more about deployment in production, I took on the responsibility of API design and deployment. I designed the RESTful API and set up the Telegram webhooks using FastAPI, wrote an in-house asynchronous module to communicate with the Telegram Bot API, and used MongoDB for data warehousing and user profiling, along with Redis for in-memory caching. We deployed the entire project on a Hetzner Ubuntu VPS using Docker and the Uvicorn ASGI server. The platform operated on a freemium business model and organically gained nearly 50k subscribers in the first three months. Our backend could handle the server's load balancing capacity, managing almost 100 requests at peak times (during market hours). The paid plan offered users access to proprietary market analysis reports, news sentiment analysis, trading signals, and custom alerts. Many of these features, such as sentiment analysis (though another team later made changes) and trading signals, were developed by me.
	- Built a trading order execution module for TSE stocks on Mofid Securities Brokerage using Selenium Chrome WebDriver and PyAutoGUI. This module could sign in to a user's account, retrieve their portfolio, and execute orders based on user input.
	- Developed a benchmarking and strategy performance simulation environment to facilitate forward and back testing and optimize trading strategies. This system simulated the actions occurring in an exchange or broker during trades or transactions, accurately accounting for commission fees, interest, penalties, and other factors.
	- Designed a real-time, automatic cryptocurrency triangular arbitrage opportunity hunter using market microstructure and order book depth analysis on the Binance exchange. The system was designed to efficiently handle and scan over 1000 trading pairs asynchronously, triggering chain trading mechanisms with a latency of under 100ms if the profits exceeded the associated fees for the complete trading cycle. The system was built in Python, utilizing multithreading for trading triggers and an asynchronous design for lossless data retrieval. It incorporated Redis for in-memory caching, Docker for containerization, and was deployed on AWS EC2 instances in the ea2 region. Unfortunately, the system failed to generate profit due to the latency in signal propagation and order execution.

Besides what I was busy with at Sepanta, I attended a summer school on "How to Write a Good Research Proposal" at Mashhad Dental School in August 2019. While the course wasn't particularly helpful, it introduced me to SPSS for statistical analysis, sparking a deeper interest in statistics and statistical analysis, which became one of my favorite problem-solving areas. Also in that year, Kerman Concrete Company asked me to create a management dashboard for his company to monitor financial and operational performance. Although I suggested using Python or MFC, they preferred Excel for its simplicity and interactive charts and tables. They offered fair compensation, so I learned VBA (Visual Basic for Applications) over two weeks and built the dashboard within a month. In 2020, I was asked me to step in as the interim instructor for a Fast-Paced Principles of Programming course. I had the opportunity to teach a class of 15 students for six months, covering topics like data structures, algorithms, flow charts, C programming, and Python programming. Parallel to my teaching experience, I enrolled in the MikroTik Certified Network Associate course and a C# and .NET programming course and dived deeper into network administration and also familiarize myself with .NET framework. In 2021, I attended the Fifth IPM Advanced School on Computing focusing on Artificial Intelligence.

From March 2023 to July 2024, I worked at CanApply as a Data Scientist and AI Engineer. CanApply is an educational technology start up based in Montreal, Canada, where I had the opportunity to develop and engineer their core AI products backend systems from design to production. At CanApply, I was focused on collecting a massive amount of data on academic institutions, recommendation systems based on that data, and engineering "Dana", a RAG chatbot application. I led the creation of a comprehensive data collection system, gathering information from over 223 Canadian universities and colleges, and built a knowledge graph using Neo4j to support data retrieval and analysis. I developed a recommendation system to help students find suitable degree programs and also an AI assistant called "Dana", for personalized study abroad guidance. During my employment at CanApply, I leveraged technologies like Python, OpenAI's GPT models, SpaCy's NLP models, Neo4j knowledge graphs, SQL (MariaDB) and NoSQL (MongoDB) databases, FastAPI, Flask, Docker, and Hetzner VPSs. Additionally, as a data scientist, I conducted market analysis and business intelligence benchmarks and attended the Collision Conference to showcase our AI products. 

Here's a detailed list of my responsibilities and accomplishments during my employment at CanApply:
	- Developed an automated data pipeline for collecting, cleaning, and processing data on Canadian educational institutions, including services, courses, fees, funding details, and admission requirements. This data was sourced from universities' websites and public educational databases. We successfully gathered information from 223 universities and colleges in Canada, covering over 13,000 programs. I also built a knowledge graph replica of the database using the Neo4j graph database for knowledge retrieval, querying, and analysis in RAG applications. The data and the graph are updated quarterly to ensure the accuracy and reliability of the information in the knowledge center. For this project, I used Python and leveraged Selenium Chrome WebDriver and Beautiful Soup for web scraping, implementing bot detection bypassing mechanisms such as proxy and user agent rotation and browser fingerprinting. I used Regular Expressions for data preprocessing and fine-tuned a SpaCy NLP model for NER and entity extraction. Additionally, I utilized the OpenAI's GPT-3.5-Turbo API for data augmentation and cleaning, MariaDB MySQL for data warehousing, and Neo4j for the graph database.
	- Designed a degree program content-filtering recommendation system on top of the knowledge center for students based on their preferences, academic background, and career goals. This system helps students and clients planning their academic journey to find and explore the most suitable degree programs offered by Canadian academic institutions. The recommendations are tailored to the fields they are interested in, the courses they prefer, their career goals, and how well the programs align with their academic background. Users can also filter the list of recommended programs to narrow down their options. The recommendation system is implemented in Python and utilizes the OpenAI's GPT Embeddings API for data categorization and semantic similarity. It uses FastAPI for RESTful API development, and SciPy, NumPy, and Pandas for matrix operations. The system is deployed on Hetzner Ubuntu VPS using Docker and the Uvicorn ASGI server. It achieved nearly 100% accuracy in the top-5 recommendations across 20 different program categories, based on a survey of 300 clients. The system maintains a response time of under 800-1000ms for recommendations and 500-600ms for filtration and pagination on production servers.
	- Engineered a GraphRAG-enabled study abroad AI assistant to assist students with their questions and offer personalized guidance for planning their academic journeys and immigration processes. This AI assistant helps students understand admission requirements for their study abroad plans by providing them with up-to-date, accurate information from the knowledge center. The chatbot is powered by OpenAI's completions and embeddings models API and uses real-time data from querying the knowledge graph database to provide contextually relevant responses throughout the conversation. Specific programs and universities mentioned in the conversation are listed in the conversation panel, allowing users to access full information on the corresponding pages on the smart platform. The system includes topic moderation, data routing, context translation, graph database Cypher query generation, and other mechanisms managed through a multi-agent architecture. The main conversational engine is built on this architecture. The system is implemented in Python, using FastAPI for RESTful API development, Neo4j for the graph database and Cypher querying language for knowledge retrieval, and Pusher for real-time completion tokens streaming. It is deployed on Hetzner Ubuntu VPS using Docker and the Uvicorn ASGI server. The RAG response latency on production servers ranges from 1800 to 7200 milliseconds, depending on the conversation's context.
	- Conducted numerous data-driven analyses and business intelligence benchmarks on the Canadian educational technology market, exploring its trends, competitors, and opportunities. I identified key market segments, potential growth areas, time-to-market strategies, and potential partnerships. I analyzed government-issued reports on international student enrollment and visa applications from 2015 to 2023 across different provinces and territories in Canada. By implementing an ARIMA forecasting model, I estimated the number of international students in Canada for the next four quarters, identifying trends in application numbers. This analysis helped report the optimal timeframes for marketing campaigns and student recruitment. I used Scikit-Learn, Pandas, NumPy, Matplotlib, and Seaborn for data analysis and visualization.
	- Attended the Collision Conference in June 2024 as an exhibitor for CanApply, where we presented our AI products and the smart platform's functionalities for study abroad planning to attendees and industry peers. 

Putting CanApply aside, recently I've been working on some personal projects. One of them is an "AI Job Hunter" which is a software designed to automate and streamline the job hunting process. The software leverages a scraping and web interaction engine to log into accounts on major job listing sites like Glassdoor, Indeed, and LinkedIn. Based on a user's preferences—including job category, title, skills, and desired locations—the software activates a search mechanism for each combination of these preferences, extracting and storing job listings along with details such as the job description, posted date, company name, company ratings, and user reviews of the company or position. The system then moderates each job's details against the user's preferences to exclude inaccurately indexed jobs or those with misleading titles or descriptions. A compatibility mechanism compares and analyzes each job's description and requirements against the user's desires and capabilities. A score out of 100 is assigned to each job based on how well it aligns with the user's preferences and needs. This scoring is done by asking various questions of the job description, each targeting a different and important aspect based on the user's resume. Similarly, the user's resume and background are evaluated, and a score out of 100 is given based on how well they meet the job's requirements. Questions are asked of the user's resume, targeting strengths or weaknesses relevant to the job, indicating the likelihood of the application being considered by the hiring team or HR office. A weighted average of these scores produces a match score, which the user can adjust according to the importance of each factor. If the match score exceeds a certain threshold, the system will apply for the job on the user's behalf, customizing the resume based on the job requirements and qualifications. This customization is limited to the information provided in the user's ultimate and complete resume; the system will not add or generate new skills or information not already present in the user's resume. Additionally, upon the user's request, the system can draft a cover letter and, if needed, an email for the application. The user can provide templates for their resume and cover letter in .docx format, and the system can automatically generate the final documents in .docx or .pdf format for attachment to the application. The system also offers a batch processing option, allowing users to process thousands of job descriptions at once. Based on the desired match score weightings and thresholds, the system automatically processes all applications and generates the necessary documents for the user. This system is powered by the OpenAI GPT-4 completions API and requires the user to input their OpenAI API key into the system environment variables. Ultimately, this system enables users to efficiently search for jobs by automating decision-making and alignment analysis, traditionally requiring significant manual effort and time.

Another personal project I've been working on is an ML-Driven Bidirectional Auto Quant Trading System which is a system focusing on identifying profitable trading segments, using them as both the target and input for the reward function of an action-critic reinforcement learning agent. The project's goal is to address the computational complexity of excluding unprofitable trading zones by benchmarking bidirectional trades between pivot points in a series of price data. This approach aims to identify the best segmentation pattern in the price data series stochastically. Although the project is still in progress, it plans to utilize an LSTM ensemble model as the baseline supervised learning model, along with careful feature engineering and selection. The final trading decisions will be determined by an action-critic agent, guided by a precisely defined reward function. The experiment is to find the impact of automatic trading zones segmentations on the quality of the RL agent's decision making by computing the profitability in a simulated environment.

To conclude, Looking ahead, my goals are clear. I aim to continue advancing in the field of machine learning and AI and hopefully, inspire and lead future innovations that can make a significant impact on technology and society. I believe in the power of technology to transform lives and am committed to being a part of that change. If you're interested to know more about what I did at Sepanta and CanApply, and also about the personal projects, I will dive more deeply into them.